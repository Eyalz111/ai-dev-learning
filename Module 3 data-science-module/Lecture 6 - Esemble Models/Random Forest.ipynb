{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Code Workflow Guide\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a Random Forest classifier to predict customer churn in a banking dataset. Below is a detailed explanation of each step in the code workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Data Preprocessing\n",
    "\n",
    "### Step 1: Import and Load Dataset\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('churn_modelling.csv')\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Imports the pandas library for data manipulation\n",
    "- Loads the CSV file into a DataFrame called `dataset`\n",
    "- The dataset contains customer information for churn prediction\n",
    "\n",
    "### Step 2: Explore the Data\n",
    "\n",
    "```python\n",
    "dataset.head()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Displays the first 5 rows of the dataset\n",
    "- Helps understand the structure and content of the data\n",
    "- Shows column names and sample values\n",
    "\n",
    "### Step 3: Check for Missing Data\n",
    "\n",
    "```python\n",
    "dataset.info()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Provides summary information about the DataFrame\n",
    "- Shows data types of each column\n",
    "- Displays count of non-null values (to identify missing data)\n",
    "- Reports memory usage\n",
    "- **Result**: Confirms there are no missing values in this dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning & Feature Engineering\n",
    "\n",
    "### Step 4: Remove Non-Predictive Columns\n",
    "\n",
    "```python\n",
    "dataset.drop(['CustomerId', 'Surname'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Removes `CustomerId` and `Surname` columns\n",
    "- `axis=1` specifies column removal (not row)\n",
    "- `inplace=True` modifies the original DataFrame\n",
    "- **Why**: These are unique identifiers with no predictive value\n",
    "\n",
    "### Step 5: Handle Geography (Categorical Variable)\n",
    "\n",
    "```python\n",
    "dataset['Geography'].unique()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Shows all unique values in the Geography column\n",
    "- **Result**: ['France', 'Germany', 'Spain']\n",
    "\n",
    "```python\n",
    "geography_dummies = pd.get_dummies(dataset['Geography'], drop_first=True)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Converts categorical Geography into binary dummy variables\n",
    "- `drop_first=True` removes one category to avoid multicollinearity\n",
    "- Creates: `Germany` and `Spain` columns (France is the reference category)\n",
    "- **Why**: Machine learning models need numerical input\n",
    "\n",
    "```python\n",
    "dataset = pd.concat([geography_dummies, dataset], axis=1)\n",
    "dataset.drop(['Geography'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- `pd.concat()` adds the dummy columns to the left of the dataset\n",
    "- `axis=1` means concatenate horizontally (add columns)\n",
    "- Removes the original `Geography` column\n",
    "- **Result**: Two new binary columns (Germany, Spain) replace Geography\n",
    "\n",
    "### Step 6: Handle Gender (Binary Categorical Variable)\n",
    "\n",
    "```python\n",
    "dataset['Gender'].unique()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Shows unique values: ['Female', 'Male']\n",
    "\n",
    "```python\n",
    "dataset['Gender'] = dataset['Gender'].apply(lambda x: 0 if x == 'Female' else 1)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Uses a lambda function to encode gender as binary\n",
    "- Female → 0, Male → 1\n",
    "- `apply()` applies the function to each row\n",
    "- **Why**: Simpler than dummy variables for binary categories\n",
    "\n",
    "---\n",
    "\n",
    "## Train-Test Split\n",
    "\n",
    "### Step 7: Separate Features and Target\n",
    "\n",
    "```python\n",
    "X = dataset.iloc[:, :-1].values\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Selects all columns except the last one (features/inputs)\n",
    "- `iloc[:, :-1]` means: all rows, all columns except last\n",
    "- `.values` converts DataFrame to numpy array\n",
    "- **X** = Independent variables (predictors)\n",
    "\n",
    "```python\n",
    "y = dataset.iloc[:, -1].values\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Selects only the last column (target variable)\n",
    "- `iloc[:, -1]` means: all rows, last column only\n",
    "- **y** = Dependent variable (what we're predicting: Exited)\n",
    "\n",
    "### Step 8: Split Data into Training and Test Sets\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Splits data into 80% training, 20% testing\n",
    "- `X_train`, `y_train`: Used to train the model\n",
    "- `X_test`, `y_test`: Used to evaluate the model on unseen data\n",
    "- `random_state=0`: Ensures reproducible splits\n",
    "- **Why**: Testing on separate data prevents overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Building and Training the Model\n",
    "\n",
    "### Step 9: Create the Random Forest Model\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Imports the Random Forest algorithm\n",
    "- Creates a model with:\n",
    "  - `n_estimators=100`: Uses 100 decision trees\n",
    "  - `max_depth=4`: Each tree has maximum 4 levels deep\n",
    "  - `random_state=0`: Ensures reproducible results\n",
    "- **Why Random Forest**: Handles non-linear patterns, resistant to overfitting\n",
    "\n",
    "### Step 10: Train the Model\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Trains the Random Forest on the training data\n",
    "- The model learns patterns from `X_train` to predict `y_train`\n",
    "- Builds 100 decision trees using random subsets of data and features\n",
    "- **Result**: Trained model ready for predictions\n",
    "\n",
    "### Step 11: Make Predictions\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Uses the trained model to predict outcomes for test data\n",
    "- `y_pred` contains predicted values (0 or 1)\n",
    "- These predictions are compared against actual values (`y_test`)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Model Evaluation\n",
    "\n",
    "### Step 12: Predict Single Customer\n",
    "\n",
    "```python\n",
    "model.predict([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Makes a prediction for one specific customer\n",
    "- Input format: `[Germany, Spain, CreditScore, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary]`\n",
    "- This customer: France (0,0), Male, 40 years old, etc.\n",
    "- **Output**: 0 (won't churn) or 1 (will churn)\n",
    "\n",
    "### Step 13: Confusion Matrix\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Creates a 2x2 matrix comparing predictions vs actual values\n",
    "- **Format**:\n",
    "  ```\n",
    "  [[True Negative   False Positive]\n",
    "   [False Negative  True Positive]]\n",
    "  ```\n",
    "- Shows where the model was correct/incorrect\n",
    "\n",
    "### Step 14: Calculate Accuracy (Manual)\n",
    "\n",
    "```python\n",
    "(1521+208)/(1521+208+74+197)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Manually calculates accuracy from confusion matrix\n",
    "- Formula: (Correct Predictions) / (Total Predictions)\n",
    "- (TN + TP) / (TN + TP + FN + FP)\n",
    "\n",
    "### Step 15: Calculate Accuracy (Automated)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Uses sklearn's built-in accuracy function\n",
    "- Compares `y_test` (actual) with `y_pred` (predicted)\n",
    "- **Result**: ~86.45% accuracy\n",
    "\n",
    "### Step 16: Cross-Validation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=model,\n",
    "                             X=X,\n",
    "                             y=y,\n",
    "                             scoring='accuracy',\n",
    "                             cv=10)\n",
    "print(f\"Average Accuracy: {accuracies.mean()*100} %\")\n",
    "print(f\"Standard Deviation: {accuracies.std()*100} %\")\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Performs 10-fold cross-validation\n",
    "- **Process**: \n",
    "  1. Splits data into 10 parts\n",
    "  2. Trains on 9 parts, tests on 1 part\n",
    "  3. Repeats 10 times with different test parts\n",
    "  4. Calculates accuracy for each fold\n",
    "- `accuracies.mean()`: Average accuracy across all folds\n",
    "- `accuracies.std()`: Variation in accuracy (consistency measure)\n",
    "- **Why**: More reliable than single train-test split\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Workflow Summary:**\n",
    "1. **Load data** → Check structure and missing values\n",
    "2. **Clean data** → Remove unnecessary columns\n",
    "3. **Encode categorical variables** → Convert text to numbers\n",
    "4. **Split data** → Training (80%) and Testing (20%)\n",
    "5. **Train model** → Random Forest learns patterns\n",
    "6. **Predict** → Generate predictions on test data\n",
    "7. **Evaluate** → Measure accuracy and reliability\n",
    "\n",
    "**Model Performance:**\n",
    "- **Accuracy**: ~86% (correctly predicts 86 out of 100 customers)\n",
    "- **Cross-validation**: Confirms model consistency across different data splits\n",
    "\n",
    "**Use Case:**\n",
    "The bank can use this model to identify customers likely to leave and take proactive retention measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmJjyDo9m_6Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('churn_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1663515333409,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "O9-X_v_znVmb",
    "outputId": "afa2cdff-525a-48c4-941b-683765018319"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LIXVmeeF3fk"
   },
   "source": [
    "### Checking missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1663515333410,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "hdcvvLRr0B6K",
    "outputId": "e6e0acd2-2ff5-45bc-e9a4-63a9bf4acb8c"
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset we are using is related to customer churn modeling. It contains information about customers\n",
    "# such as their credit score, geography, gender, age, tenure, balance, number of products, whether they have\n",
    "# a credit card, whether they are active members, their estimated salary, and whether they exited (churned).\n",
    "# \n",
    "# Our goal is to preprocess this data and build a machine learning model to predict customer churn. We will\n",
    "# handle missing data, encode categorical variables, and then use ensemble models like Random Forest and\n",
    "# XGBoost to train and evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Handling categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNqE5cx0JDRX"
   },
   "source": [
    "CustomerId and Surname columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq6RyW6MIcm8"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['CustomerId', 'Surname'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1663515333411,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "yWryZKZnJCkv",
    "outputId": "42b92e99-5360-46ad-ac6e-bae2e5f248e0"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gibNetsJvtB"
   },
   "source": [
    "Geography column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1663515333411,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "BhXtJiDOJU_1",
    "outputId": "894b7d98-f063-4c7b-d9e0-6a292baeec7e"
   },
   "outputs": [],
   "source": [
    "dataset['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVSDsa5iJtMO"
   },
   "outputs": [],
   "source": [
    "geography_dummies = pd.get_dummies(dataset['Geography'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1663515333412,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "v1cKMGiVKfr8",
    "outputId": "a75adba2-d547-4f44-b7f2-bb4f5a90f4ae"
   },
   "outputs": [],
   "source": [
    "geography_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQHSvIuBK4MX"
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([geography_dummies, dataset], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1663515333413,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "NiydpPB_LcQ0",
    "outputId": "018a9f6c-5f70-49fd-f6c4-b23155a91d5f"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLns7VQALksb"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['Geography'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1663515333414,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "pV_cQGxNL7hG",
    "outputId": "dd0d155e-d09b-4f93-951e-d63003b1308e"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KADgg0uDNRtQ"
   },
   "source": [
    "Gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1663515333414,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "Oraro5G0MJ5B",
    "outputId": "4ae825a1-68a5-4ddc-dd29-12cdbac01695"
   },
   "outputs": [],
   "source": [
    "dataset['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgfkgPJHMXPj"
   },
   "outputs": [],
   "source": [
    "dataset['Gender'] = dataset['Gender'].apply(lambda x: 0 if x == 'Female' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1663515333956,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "bnn1WzhiNBqj",
    "outputId": "33e9bb1e-8af1-4ad5-8799-2b6f3ed22348"
   },
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiKCmjPnOdym"
   },
   "source": [
    "### Creating the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5NVubxeOiPM"
   },
   "source": [
    "Getting the inputs and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6-BzqKhEBiT"
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G702vTzaEC3x"
   },
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1663515333958,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "3B1qfJ5NO8BQ",
    "outputId": "1e6ee753-9943-43e6-8890-e8fbed3d803f"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1663515333958,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "CR9SOG09O9RM",
    "outputId": "61fb9952-4341-41f5-bf32-3d97de07e714"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSKRpuJ6XNup"
   },
   "source": [
    "Getting the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu4KkT5EFDtA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mN88WZuYmXq"
   },
   "source": [
    "## Part 2 - Building and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-wsRF2zYnGC"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x0Tg-NDWJ2T"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rVZpyt7Y4qg"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1663515335594,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "2CtWNl1NWTtu",
    "outputId": "4f62a36d-1695-4309-ac8c-53199bbce50c"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otleXDFDZH11"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n5uerjKWYef"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1663515335595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "KJ-jbNumW1oa",
    "outputId": "409063cd-6014-4906-b1de-e360a2156340"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1663515335595,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "Yfi3lQ0Xbqsj",
    "outputId": "2b314ec2-2451-4bed-86c1-37de3168418a"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84QFoqGYeXHL"
   },
   "source": [
    "### Predicting the result of a single observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGRo3eacgDdC"
   },
   "source": [
    "**Homework**\n",
    "\n",
    "Use our model to predict if the customer with the following informations will leave the bank:\n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: \\$ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: \\$ 50000\n",
    "\n",
    "So, should we say goodbye to that customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1663515335596,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "kX-pv9TMdFwT",
    "outputId": "6aa64732-c1b0-414e-8017-a3ecb0bb05f7"
   },
   "outputs": [],
   "source": [
    "model.predict([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Part 3: Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0oyfLWoaEGw"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1663515335596,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "olDlHz4KwfC0",
    "outputId": "99aeb222-11a6-4b90-b6d1-8b1ed15c1cb7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l_6GuspajAC"
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1663515335596,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "N_m2QvdVx20S",
    "outputId": "1bf92ad1-59e8-444b-92d6-d30bbb407fad"
   },
   "outputs": [],
   "source": [
    "(1521+208)/(1521+208+74+197)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1663515335597,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "AwPSlrVbycVw",
    "outputId": "9a1cb91e-e337-4f1b-f6d8-019cf0e975eb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jn64zWSbiQg"
   },
   "source": [
    "### k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11615,
     "status": "ok",
     "timestamp": 1663515347205,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "sj8bdhSn1rg2",
    "outputId": "1a296ee2-cf42-45d9-daa8-9447e2c8bf9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = model,\n",
    "                             X = X,\n",
    "                             y = y,\n",
    "                             scoring = 'accuracy',\n",
    "                             cv = 10)\n",
    "print(f\"Average Accuracy: {accuracies.mean()*100} %\")\n",
    "print(f\"Standard Deviation: {accuracies.std()*100} %\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCjzvTDW6YYvKWbHWkiVkp",
   "provenance": [
    {
     "file_id": "1r7hRQSlIB-ZEaWi32hIqLjeANm7L-I1U",
     "timestamp": 1661836011102
    },
    {
     "file_id": "1C29XcV-ykBUe9MVmlwPBBbh4IGtA-p9y",
     "timestamp": 1652624065506
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
