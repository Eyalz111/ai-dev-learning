{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”§ Data Preprocessing Pipeline\n",
    "*Professional Guide to Machine Learning Data Preparation*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Overview\n",
    "\n",
    "Data preprocessing is a critical phase in any machine learning project. This pipeline includes data cleaning, handling missing values, encoding categorical variables, feature scaling, and outlier detection. These steps ensure high-quality data input and ultimately lead to optimal model performance.\n",
    "\n",
    "**Workflow:** Raw Data â†’ Clean Data â†’ Scaled Features â†’ Encoded Variables â†’ Model Ready\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Process\n",
    "\n",
    "### ðŸ”´ Step 1: Handling Missing Data\n",
    "**Replace missing values (NaN) with column means using SimpleImputer for numerical features**\n",
    "\n",
    "```python\n",
    "# Import SimpleImputer for handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create imputer with mean strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Fit and transform numerical columns (1 and 2)\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Applied to numerical columns only\n",
    "- Uses mean imputation strategy\n",
    "- Preserves data distribution\n",
    "- Alternative strategies: median, most_frequent\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ  Step 2: Feature Scaling\n",
    "**Normalize numerical features to 0-1 range using MinMaxScaler to improve model performance**\n",
    "\n",
    "```python\n",
    "# Import MinMaxScaler for feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale numerical features to [0,1] range\n",
    "X[:, 1:3] = scaler.fit_transform(X[:, 1:3])\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Scales features to [0,1] range\n",
    "- Prevents feature dominance\n",
    "- Improves algorithm convergence\n",
    "- Essential for distance-based algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ¢ Step 3: Categorical Data Encoding\n",
    "**Convert categorical variables to numerical format using One-Hot Encoding**\n",
    "\n",
    "```python\n",
    "# Import required classes for categorical encoding\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create column transformer with one-hot encoder\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(), [0])], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply transformation and convert back to numpy array\n",
    "X = np.array(ct.fit_transform(X))\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Creates binary columns for each category\n",
    "- Prevents ordinal assumptions\n",
    "- Applied to first column (index 0)\n",
    "- Other columns remain unchanged\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŸ£ Step 4: Outlier Detection & Removal\n",
    "**Identify and filter outliers using the IQR (Interquartile Range) method**\n",
    "\n",
    "```python\n",
    "# Calculate quartiles for outlier detection\n",
    "Q1 = np.percentile(X, 25, axis=0)\n",
    "Q3 = np.percentile(X, 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds using 1.5 * IQR rule\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Create mask to filter outliers\n",
    "mask = ((X >= lower_bound) & (X <= upper_bound)).all(axis=1)\n",
    "\n",
    "# Apply filter to both features and target\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses statistical IQR method\n",
    "- 1.5 * IQR rule for bounds\n",
    "- Removes extreme values\n",
    "- Maintains data quality\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Benefits\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|------------|\n",
    "| **ðŸŽ¯ Efficiency** | Streamlined pipeline for complex data processing |\n",
    "| **ðŸ” Accuracy** | Noise reduction and data quality improvement |\n",
    "| **âš¡ Performance** | Optimized data for machine learning algorithms |\n",
    "| **ðŸ›¡ï¸ Reliability** | Robust preprocessing for consistent results |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'PreprocessData.csv' into a pandas DataFrame\n",
    "dataset = pd.read_csv('PreprocessData.csv')\n",
    "\n",
    "# Extract all columns except the last one from the dataset and convert them to a NumPy array, assigning it to X\n",
    "X = dataset.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column from the dataset and convert it to a NumPy array, assigning it to y\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hCsz2yCebe1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYrOQ43XcJR3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 1, 0, 7, 8, 9, 1, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## 1. Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c93k7ipkSexq"
   },
   "outputs": [],
   "source": [
    "# Import the SimpleImputer class from sklearn to handle missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object that will replace missing values (NaN) with the mean of the column\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Fit the imputer to the columns 1 and 2 of X to compute the mean of these columns\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# Transform the columns 1 and 2 of X by replacing missing values with the computed mean\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3UgLdMS_bjq_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\nogas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\nogas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\nogas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nogas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nogas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale only the numerical columns (columns 1 and 2) in X\n",
    "X[:, 1:3] = scaler.fit_transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 0.7391304347826089, 0.6857142857142855],\n",
       "       ['Spain', 0.0, 0.0],\n",
       "       ['Germany', 0.1304347826086958, 0.17142857142857149],\n",
       "       ['Spain', 0.4782608695652175, 0.37142857142857144],\n",
       "       ['Germany', 0.5652173913043479, 0.45079365079365075],\n",
       "       ['France', 0.34782608695652173, 0.2857142857142856],\n",
       "       ['Spain', 0.5120772946859904, 0.11428571428571432],\n",
       "       ['France', 0.9130434782608696, 0.8857142857142857],\n",
       "       ['Germany', 1.0, 1.0],\n",
       "       ['France', 0.43478260869565233, 0.5428571428571427]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## 3. Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5hwuVddlSwVi"
   },
   "outputs": [],
   "source": [
    "# Import ColumnTransformer from sklearn to apply transformations to specific columns of the data.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Import OneHotEncoder from sklearn to perform one-hot encoding on categorical data.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a ColumnTransformer object that applies one-hot encoding to the first column (index 0) of X.\n",
    "# The remainder='passthrough' argument ensures that all other columns remain unchanged.\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "# Fit the ColumnTransformer to X and transform X, converting the result back into a NumPy array.\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "f7QspewyeBfx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 0.7391304347826089 0.6857142857142855]\n",
      " [1.0 0.0 0.0 1.0 0.0 0.0]\n",
      " [1.0 0.0 1.0 0.0 0.1304347826086958 0.17142857142857149]\n",
      " [1.0 0.0 0.0 1.0 0.4782608695652175 0.37142857142857144]\n",
      " [1.0 0.0 1.0 0.0 0.5652173913043479 0.45079365079365075]\n",
      " [0.0 1.0 0.0 0.0 0.34782608695652173 0.2857142857142856]\n",
      " [1.0 0.0 0.0 1.0 0.5120772946859904 0.11428571428571432]\n",
      " [0.0 1.0 0.0 0.0 0.9130434782608696 0.8857142857142857]\n",
      " [1.0 0.0 1.0 0.0 1.0 1.0]\n",
      " [0.0 1.0 0.0 0.0 0.43478260869565233 0.5428571428571427]]\n",
      "\n",
      "\n",
      "  France Germany Spain Age_Scaled Salary_Scaled Extra_Column\n",
      "0    0.0     1.0   0.0        0.0       0.73913     0.685714\n",
      "1    1.0     0.0   0.0        1.0           0.0          0.0\n",
      "2    1.0     0.0   1.0        0.0      0.130435     0.171429\n",
      "3    1.0     0.0   0.0        1.0      0.478261     0.371429\n",
      "4    1.0     0.0   1.0        0.0      0.565217     0.450794\n",
      "5    0.0     1.0   0.0        0.0      0.347826     0.285714\n",
      "6    1.0     0.0   0.0        1.0      0.512077     0.114286\n",
      "7    0.0     1.0   0.0        0.0      0.913043     0.885714\n",
      "8    1.0     0.0   1.0        0.0           1.0          1.0\n",
      "9    0.0     1.0   0.0        0.0      0.434783     0.542857\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "# Convert the array to a pandas DataFrame with appropriate column names\n",
    "import pandas as pd\n",
    "\n",
    "# Create column names based on the data structure\n",
    "# First 3 columns are one-hot encoded country names, followed by scaled Age and Salary\n",
    "# Added 'Extra_Column' to match the 6 columns in X\n",
    "column_names = ['France', 'Germany', 'Spain', 'Age_Scaled', 'Salary_Scaled', 'Extra_Column']\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\n\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### 4. Check and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical computations.\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the first quartile (25th percentile) of each column in X.\n",
    "# This identifies the lower boundary for potential outliers.\n",
    "Q1 = np.percentile(X, 25, axis=0)\n",
    "\n",
    "# Calculate the third quartile (75th percentile) of each column in X.\n",
    "# This identifies the upper boundary for potential outliers.\n",
    "Q3 = np.percentile(X, 75, axis=0)\n",
    "\n",
    "# Compute the interquartile range (IQR) by subtracting Q1 from Q3.\n",
    "# The IQR measures statistical dispersion and helps in outlier detection.\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine the lower bound for outlier detection by subtracting 1.5 times the IQR from Q1.\n",
    "# Values below this threshold are considered outliers.\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "# Determine the upper bound for outlier detection by adding 1.5 times the IQR to Q3.\n",
    "# Values above this threshold are considered outliers.\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Create a mask to filter out outliers from X.\n",
    "# Include only rows where all column values are within the lower and upper bounds.\n",
    "mask = ((X >= lower_bound) & (X <= upper_bound)).all(axis=1)\n",
    "X = X[mask]\n",
    "\n",
    "# Filter y to match the length of the filtered X_train.\n",
    "# This ensures y_train corresponds to the samples in X_train.\n",
    "y = y[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 0.0, 0.7391304347826089, 0.6857142857142855],\n",
       "       [1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.1304347826086958, 0.17142857142857149],\n",
       "       [1.0, 0.0, 0.0, 1.0, 0.4782608695652175, 0.37142857142857144],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.5652173913043479, 0.45079365079365075],\n",
       "       [0.0, 1.0, 0.0, 0.0, 0.34782608695652173, 0.2857142857142856],\n",
       "       [1.0, 0.0, 0.0, 1.0, 0.5120772946859904, 0.11428571428571432],\n",
       "       [0.0, 1.0, 0.0, 0.0, 0.9130434782608696, 0.8857142857142857],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       "       [0.0, 1.0, 0.0, 0.0, 0.43478260869565233, 0.5428571428571427]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 1, 0, 7, 8, 9, 1, 5, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOihYlX/ooG5h+qw0sLIjn8",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
